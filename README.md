# FinJect - Locating App For Visually Impaired Users
**Overview**
FinJect is an assistive technology designed to aid visually impaired users in locating objects within a confined space. It combines object tagging using Bluetooth Low Energy (BLE) tags and image detection using a YOLO model integrated with text-to-speech recognition.

**Features**
**Object Tagging:** 
**->** Objects such as bottles, wallets, keys, etc., are tagged with BLE tags. 
**->** These tags emit a distinct sound when triggered by voice command, aiding visually impaired users in locating them based on auditory cues.

**Image Detection:**
**->** Utilizes a YOLO (You Only Look Once) model with text-to-speech capabilities. 
**->** The smartphone camera captures the surroundings and describes the objects or people present, providing real-time auditory feedback to the user.

**Development Highlights**
**Innovative Technology:** 
**->** Developed FinJect to offer a novel solution for visually impaired individuals, integrating RFID tagging and image recognition technologies.

**Enhanced Image Labeling:** 
**->** Implemented R-NN algorithms to improve the accuracy of image labeling from 81% to 95%, ensuring reliable object recognition.

**RFID Tag Integration:**
**->** Integrated BLE tags to simplify the process of locating belongings, enhancing user independence and efficiency.

**Design Principles**
**User-Centric Approach:** 
**->** Prioritized intuitive design to ensure FinJect is user-friendly and accessible, focusing on the needs of visually impaired users.

**Cost-Effective Solution:**
**->** Developed in a cost-effective manner to make assistive technology more accessible to a wider user base.

**Recognition: Published by Intellectual Property India in 2021, highlighting FinJect's significant contribution to assistive technology.
**
**Installation and Usage**
**Requirements:** Compatible smartphone with BLE capability, installed FinJect app.
**Setup:** Attach BLE tags to commonly used objects. Launch the FinJect app and follow voice prompts to trigger tags or use the camera for image detection.

**Usage:** Navigate through the environment using auditory cues from triggered tags or real-time descriptions from the camera feed.
